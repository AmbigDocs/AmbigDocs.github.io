<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AmbigDocs</title>

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link href="./css/style.css" rel="stylesheet">
    <!-- Favicons -->
  </head>
  <body>
    <nav id="navbar" class="shadow-sm navbar navbar-light bg-light">
      <div id="navbar-content" class="container">
        <a class="navbar-brand" href="#">AmbigDocs</a>
        <ul class="nav nav-pills">
          <li class="nav-item"><a href="#" class="nav-link active" aria-current="page">Home</a></li>
          <li class="nav-item"><a href="./explorer.html" class="nav-link">Data Explorer</a></li>
        </ul>
      </div>
    </nav>
    <main id="main" class="container fs-5">
      <div class="p-5">
        <h1 class="text-center display-5 fw-normal">AmbigDocs: Reasoning across Documents on Different Entities under the Same Name</h1>
        <h4 class="text-center display-10">
          <a href="https://lilys012.github.io" style="text-decoration: none;">Yoonsang Lee</a>, 
          <a href="https://www.cs.utexas.edu/~xiye/" style="text-decoration: none;">Xi Ye</a>, 
          <a href="https://eunsol.github.io" style="text-decoration: none;">Eunsol Choi</a>
      </div>
      <div class="pt-3">
        <h2 class="display-6 fw-bold">About</h2>
        <div class="row">
          <div>
            <p>Different entities with the same name can be difficult to distinguish. Handling confusing entity mentions is a crucial skill for language models (LMs). For example, given the question "Where was Michael Jordan educated?" and a set of documents discussing different people named Michael Jordan, can LMs distinguish entity mentions to generate a cohesive answer? To test this ability, we introduce a new benchmark, AmbigDocs. By leveraging Wikipedia's disambiguation pages, we identify a set of documents, belonging to different entities who share an ambiguous name. From these documents, we generate questions containing an ambiguous name and their corresponding sets of answers. Our analysis reveals that current state-of-the-art models often yield ambiguous answers or incorrectly merge information belonging to different entities. We establish an ontology categorizing four types of incomplete answers and automatic evaluation metrics to identify such categories. We lay the foundation for future work on reasoning across multiple documents with ambiguous entities.</p>
          </div>
        </div>
        <div style="display:flex; justify-content: center;">
          <a class="btn btn-primary btn-lg" type="button" href="https://arxiv.org/abs/2404.12447" target="_blank">Read the Paper</a> &nbsp;
          <a class="btn btn-success btn-lg" type="button" href="https://github.com/lilys012/AmbigDocs" target="_blank">Download Code</a> &nbsp;
          <a class="btn btn-warning btn-lg" type="button" href="https://huggingface.co/datasets/yoonsanglee/AmbigDocs" target="_blank">Download Data</a>
        </div>
      </div><br>
      <div class="pt-3">
        <h2 class="display-6 fw-bold">Types of Model Generated Answer</h2>
        <div class="row">
          <p>We develop an ontology of five types of answers that captures the unique challenge in AmbigDocs.</p>
          <ul style="padding-left:40px">
            <li><b>Complete Answer</b>: LM correctly generates all valid answers in the provided document set. Each answer is paired with the disambiguated entity name.</li>
            <li><b>Partial Answer</b>: LM generates one or more (but not all) of the valid answers in the provided document set. Each answer is paired with the disambiguated entity name.</li>
            <li><b>No Answer</b>: LM abstains from providng any answer.</li>
            <li><b>Ambiguous Answer</b>: LM generates one of the valid answers without providing the disambiguated entity name.</li>
            <li><b>Merged Answer</b>: LM generates multiple answers without providing the disambiguated entity name (merging facts about multiple disambiguated entities).</li>
          </ul>
          <div style="display:flex; justify-content: center; width: 100%">
            <img id="intro_fig" src="src/overview.png" alt="intro figure" style="max-width:70%;">
          </div> 
        </div>
      </div>
      <div class="pt-3">
        <h2 class="display-6 fw-bold">Performance of Current Models on AmbigDocs</h2>
        <div class="row">
          <p>Both open-source LLMs and GPT models suffer at identifying different entities, failing to generate complete answers. Mistral and GPT-4 exhibit the strongest performance, followed by Llama2-7b model, while larger models (Llama2-13b, GPT-3.5) significantly perform worse. Mistral generates more Partial answers but fewer Merged answers compared to GPT-4, leading to similar overall performance. Larger models such as Llama2-13b and GPT-3.5 tend to generate more Ambiguous and Merged answers rather than Complete answers, resulting in poor overall performance.</p>
        </div>
      </div>
      <div class="pt-3">
        <h2 class="display-6 fw-bold">FAQ</h2>
        <div class="row">
          <p><b>Q. How is AmbigDocs different from existing datasets like AmbigQA?</b><br>A. Despite rich study in ambiguous question answering (QA), few work studied how LMs reason when provided with a confusing document set. The major obstacle has been the lack of annotated documents containing gold disambiguated entity-answer pairs. To enable research in this area, we construct a synthetic dataset, whose single instance consists of a question asking about an ambiguous entity and a list of gold document-answer pairs for each disambiguated entity.</p>
          <p><b>Q. How is the dataset generated?</b><br>A. We identify a surface name and a list of disambiguated entities from Wikipedia's disambiguation pages. We select two documents for generating a question and their corresponding answers. Subsequently, we gather additional answers from the remaining documents.
            <div style="display:flex; justify-content: center;">
              <img id="gen_fig" src="src/generation.png" alt="generation figure" style="width:70%">
            </div> <br>
          </p>
          <p><b>Q. What is the statistics of AmbigDocs?</b><br>A. We collected a total of 36,098 examples, where they are randomly split into train/dev/test splits with ratios of 60%/10%/30%, respectively. On average, each question has 2.92 answers, covering a total of 102,624 distinct entities.</p>
        </div>
      </div>
      <div class="pt-5">
        <h2 class="display-6 fw-bold">Citations</h2>
        <p>If you find our work helpful, please cite us.</p>
        <pre class="citations border bg-light p-2 fs-6">
@article{lee2024ambigdocs,
  title={AmbigDocs: Reasoning across Documents on Different Entities under the Same Name},
  author={Lee, Yoonsang and Ye, Xi and Choi, Eunsol},
  journal={arXiv preprint arXiv:2404.12447},
  year={2024}
}</pre>

      </div>
      <div class="pt-4">
        <h2 class="display-6 fw-bold">Contact</h2>
        <p>
        For any questions, please contact <a href="https://lilys012.github.io/">Yoonsang Lee</a> or open a <a href="https://github.com/lilys012/AmbigDocs">github issue</a>.
        </p>
      </div>
      <hr>
      <p style="font-size:12px">We thank <a href="https://situatedqa.github.io/">SituatedQA</a> authors for sharing templates for generating this webpage.</p>
    </main>
    <!-- Bootstrap Bundle -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
  </body>
</html>
